ID,Vulnerability Name,Description,Category,Severity Level,Risk Score,Affected Models,Impact,Exploit Examples,Mitigation Strategies,Mitigation Tools,Source
1,Unauthorized Access,May allow attackers to alter or control model behavior.,Input Manipulation,Low,4.5,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"In 2019, a misconfiguration in Facebook's platform allowed unauthorized third-party apps to access user data without proper permissions, leading to data breaches.",Implement robust authentication and authorization mechanisms; regularly audit and update access controls.,Identity and Access Management (IAM) systems like Okta or AWS IAM.,OWASP LLM AI Security Guide
2,Weak Data Governance,Results in data tampering or model corruption.,Input Manipulation,High,7.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Equifax's 2017 data breach was partly due to poor data governance, where sensitive data was not adequately protected or monitored.",Establish clear data governance policies; implement data classification and encryption.,Data governance platforms like Collibra or Informatica.,MITRE ATT&CK for AI
3,Unvalidated Inputs,Can lead to unauthorized actions or data disclosure.,Input Manipulation,Medium,8.1,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"SQL injection attacks occur when user inputs are not properly validated, leading to unauthorized database access.",Implement input validation and sanitization; use parameterized queries.,Web Application Firewalls (WAF) like ModSecurity.,ISO/IEC 27001
4,Weak Model Parameter Encryption,Weakens overall security posture of llm deployments.,Model Exploitation,Critical,4.7,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"If machine learning model parameters are stored without encryption, attackers can steal and misuse the model, as happened with some proprietary models being leaked online.",Encrypt model parameters at rest and in transit; use secure key management systems.,Encryption libraries like PyCrypto or AWS Key Management Service (KMS).,MITRE ATT&CK for AI
5,Improper Rate Limiting,Potential disruption of service or data manipulation.,API Security,High,8.8,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"APIs without rate limiting can be subjected to denial-of-service attacks, as experienced by GitHub in a 2018 DDoS attack.",Implement rate limiting on APIs and services; monitor traffic for anomalies.,API gateways like Kong or AWS API Gateway with built-in rate limiting features.,ISO/IEC 27001
6,Prompt Injection,Weakens overall security posture of llm deployments.,Adversarial Attack,High,9.1,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Attackers manipulate AI model prompts to produce unauthorized actions or expose sensitive data—for example, tricking a chatbot into revealing confidential information.",Implement strict prompt validation and sanitization; restrict the model's output capabilities.,Input validation libraries and AI safety frameworks like OpenAI's policy guidelines.,NIST AI RMF
7,Insufficient Privacy Controls,Results in data tampering or model corruption.,Access Control,High,8.8,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Failure to anonymize user data can lead to privacy breaches, as seen in the Netflix Prize data release where anonymized data was de-anonymized.",Apply data anonymization and pseudonymization techniques; comply with privacy regulations like GDPR.,Privacy management tools like OneTrust or Privitar.,NIST AI RMF
8,Unrestricted LLM Functions,Leads to model failure or inaccurate responses.,Input Manipulation,Low,8.9,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Allowing a Language Model (LLM) to execute code can lead to execution of malicious code if inputs are not controlled.,Limit the functionalities of the LLM to necessary features only; implement execution sandboxing.,Secure sandbox environments like Docker containers.,OWASP LLM AI Security Guide
9,Poor Model Monitoring,Results in data tampering or model corruption.,Data Leakage,High,7.1,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Without proper monitoring, model drifts can go unnoticed, leading to degraded performance—for instance, a fraud detection model becoming less effective over time due to changes in fraud patterns.",Set up continuous model monitoring for performance and anomalies; retrain models as needed.,Monitoring platforms like Prometheus or AI-specific tools like Evidently AI.,ISO/IEC 27001
10,Undetected Model Modification,Potential disruption of service or data manipulation.,API Security,Medium,6.2,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"An attacker subtly alters a model to produce incorrect outputs, such as modifying a medical diagnosis model to give wrong diagnoses.",Implement integrity checks and version control for models; use cryptographic hashes to verify models.,Version control systems like Git and checksum tools like SHA256 verification.,NIST AI RMF
11,Model Parameter Exposure,Potential disruption of service or data manipulation.,Data Leakage,Critical,7.8,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"Exposure of model parameters can leak intellectual property or sensitive data embedded in the model, as happened with some AI models inadvertently revealing training data.",Securely store model parameters with encryption; limit access based on the principle of least privilege.,Secure storage solutions like AWS Secrets Manager.,NIST AI RMF
12,Model Overload,Leads to model failure or inaccurate responses.,Model Exploitation,High,8.8,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"Excessive requests to an AI model can lead to denial of service—for example, overwhelming a chatbot with requests until it becomes unresponsive.",Implement request throttling and load balancing; scale resources automatically based on demand.,"Load balancers like Nginx, cloud scaling features like AWS Auto Scaling.",ISO/IEC 27001
13,Lack of Model Integrity Verification,Leads to model failure or inaccurate responses.,Configuration Error,Critical,5.4,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Deploying models without verifying their integrity can lead to the use of tampered models—for instance, downloading and using a compromised open-source model.",Verify model integrity before deployment; use signed certificates for models.,Digital signature tools like GPG.,MITRE ATT&CK for AI
14,Insecure Data Storage,Potential disruption of service or data manipulation.,Model Manipulation,Critical,5.4,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Storing sensitive data without encryption led to the 2012 LinkedIn password leak where millions of passwords were exposed.,Encrypt data at rest and in transit; use secure databases and storage solutions.,Encryption services like BitLocker or AWS Encryption Services.,MITRE ATT&CK for AI
15,Training Data Replay,Potential disruption of service or data manipulation.,Input Manipulation,Low,5.1,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"An attacker gains access to training data by querying the model and reconstructing the data, potentially exposing sensitive information.",Limit the amount of information the model can output; use differential privacy techniques.,Differential privacy libraries like TensorFlow Privacy.,NIST AI RMF
16,Reinforcement Loop Exploitation,May allow attackers to alter or control model behavior.,Input Manipulation,Low,9,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"Manipulating feedback loops in recommendation systems to promote certain content, as seen with some social media platforms inadvertently promoting extremist content.",Monitor and adjust feedback mechanisms; implement checks for content quality.,Monitoring tools and content filters powered by AI moderation services.,OWASP LLM AI Security Guide
17,Unvalidated Inputs,Leads to model failure or inaccurate responses.,Adversarial Attack,Low,4.6,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Unvalidated inputs can lead to various injection attacks, such as cross-site scripting (XSS) in web applications.",Implement comprehensive input validation and sanitization; use allowlists for acceptable inputs.,"Security libraries and frameworks with built-in input validation, like OWASP ESAPI.",ISO/IEC 27001
18,Manipulative Model Retraining,Exposure of sensitive information or personal data.,Configuration Error,Critical,6.5,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"Feeding manipulated data to retrain a model, causing it to behave undesirably—such as Tay, Microsoft's chatbot that learned offensive language from users.",Validate and filter training data; monitor model updates closely.,Data validation tools and model monitoring platforms like Weights & Biases.,NIST AI RMF
19,Limited Model Generalization,Potential disruption of service or data manipulation.,Model Exploitation,Critical,5.7,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"A model trained on insufficient data performs poorly on real-world data, like a facial recognition system that doesn't recognize diverse faces.",Ensure diverse and comprehensive training data; use cross-validation techniques.,Data augmentation tools and machine learning frameworks like scikit-learn.,NIST AI RMF
20,Unvalidated Inputs,Can lead to unauthorized actions or data disclosure.,Model Exploitation,High,8.8,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,Unvalidated inputs may also lead to buffer overflow attacks in certain applications.,Implement strict input size limits; validate data types and formats.,Input validation libraries and security-focused coding practices.,NIST AI RMF
21,Incorrect Model Configuration,Can lead to unauthorized actions or data disclosure.,Model Exploitation,Medium,7.8,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"Misconfigured models can expose debug information or operate insecurely—for instance, running a model in debug mode in production.",Follow best practices for configuration management; use configuration files and environment variables securely.,Configuration management tools like Ansible or Chef.,OWASP LLM AI Security Guide
22,Cache Poisoning,Results in data tampering or model corruption.,Output Manipulation,Critical,8,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"An attacker poisons the cache with malicious data, leading to incorrect responses, as seen in some DNS cache poisoning attacks.",Validate cached data integrity; implement cache eviction policies.,Secure caching solutions like Redis with security configurations.,NIST AI RMF
23,Tampered Model Files,Potential disruption of service or data manipulation.,API Security,Critical,4.8,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Downloading AI models from untrusted sources that contain malware or backdoors.,Download models from trusted sources; verify file integrity with checksums.,Checksum verification tools and trusted repositories like PyPI or official GitHub repositories.,ISO/IEC 27001
24,Excessive Data Collection,Exposure of sensitive information or personal data.,Output Manipulation,Critical,7.6,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Collecting more user data than necessary leads to privacy concerns and regulatory penalties, such as fines imposed under GDPR.",Collect only necessary data (data minimization); be transparent with users about data collection.,Compliance management tools like TrustArc.,MITRE ATT&CK for AI
25,Privacy Policy Violation,Exposure of sensitive information or personal data.,Model Manipulation,High,7.4,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Not adhering to stated privacy policies, resulting in legal action—like the FTC's action against Facebook for privacy violations.",Regularly review and comply with privacy policies; train staff on privacy practices.,Privacy compliance tools and legal consultation services.,NIST AI RMF
26,LLM Replay Attacks,Leads to model failure or inaccurate responses.,Input Manipulation,High,8.1,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Replaying previous inputs to an LLM to extract sensitive information it may have processed before.,Implement policies to prevent the model from retaining session data; use stateless designs where possible.,Secure session management practices and AI frameworks with privacy features.,MITRE ATT&CK for AI
27,User Privacy Breach,Exposure of sensitive information or personal data.,Model Manipulation,Low,7.9,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Leakage of personal user data through AI outputs—for example, a chatbot revealing another user's information.",Implement strict access controls and data handling policies; use data anonymization techniques.,Data Loss Prevention (DLP) tools like Symantec DLP.,MITRE ATT&CK for AI
28,Logging Failures,Leads to model failure or inaccurate responses.,API Security,Low,8.4,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Improper logging can either miss security incidents or log sensitive data—for example, logging full credit card numbers in plaintext.",Implement proper logging practices with sensitive data masking; monitor logs for anomalies.,Logging tools like ELK Stack with security configurations.,MITRE ATT&CK for AI
29,Broken Authorization,Weakens overall security posture of llm deployments.,Data Leakage,Low,5.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Users gaining access to unauthorized resources, as seen in some API vulnerabilities where token validation is flawed.",Implement robust authorization checks; use secure token mechanisms.,Authorization frameworks like OAuth 2.0 and OpenID Connect.,MITRE ATT&CK for AI
30,Security Gap in Model API,Leads to model failure or inaccurate responses.,Input Manipulation,Critical,4.8,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"APIs exposing model functions without proper security can be exploited, as happened with some exposed AWS APIs.",Secure APIs with authentication and authorization; implement input validation.,API security tools like OWASP ZAP or API gateways with security features.,ISO/IEC 27001
31,Overfitting Exploitation,Can lead to unauthorized actions or data disclosure.,Configuration Error,Critical,5.5,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Models that overfit may reveal training data when queried in specific ways, potentially exposing sensitive information.",Use regularization techniques during training; validate model performance on unseen data.,Machine learning libraries with regularization options like scikit-learn or TensorFlow.,NIST AI RMF
32,Prompt Injection,Exposure of sensitive information or personal data.,Access Control,Critical,7.3,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Attackers manipulate AI model prompts to produce unauthorized actions or expose sensitive data.,Implement strict prompt validation and sanitization; restrict the model's output capabilities.,Input validation libraries and AI safety frameworks.,MITRE ATT&CK for AI
33,Insufficient Privacy Controls,Weakens overall security posture of llm deployments.,Output Manipulation,Medium,5.9,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Failure to anonymize user data can lead to privacy breaches, as seen in the Netflix Prize data release where anonymized data was de-anonymized.",Apply data anonymization and pseudonymization techniques; comply with privacy regulations like GDPR.,Privacy management tools like OneTrust or Privitar.,ISO/IEC 27001
34,Session Hijacking,Leads to model failure or inaccurate responses.,Configuration Error,High,4,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Attackers steal session tokens to impersonate users, as seen in various web application attacks.",Implement secure session management; use HTTPS and secure cookies.,"Web frameworks that support secure sessions, like Express.js with helmet middleware.",MITRE ATT&CK for AI
35,Unrestricted LLM Functions,May allow attackers to alter or control model behavior.,Access Control,Medium,4.2,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Allowing a Language Model (LLM) to execute code can lead to execution of malicious code if inputs are not controlled.,Limit the functionalities of the LLM; implement execution sandboxing.,Secure sandbox environments like Docker containers.,OWASP LLM AI Security Guide
36,Lack of Governance,Results in data tampering or model corruption.,Output Manipulation,Critical,4.8,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Without proper AI governance, models may be deployed with biases or without compliance checks, leading to ethical and legal issues.",Establish AI governance frameworks; conduct regular audits and compliance checks.,AI governance platforms like IBM OpenPages or SAS Compliance Solutions.,MITRE ATT&CK for AI
37,Data Retention Issues,Potential disruption of service or data manipulation.,Input Manipulation,Low,8.2,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"Storing data longer than necessary increases risk, as in cases where old user data is breached due to insufficient deletion policies.",Implement data retention and deletion policies; automate data lifecycle management.,Data management tools like Apache Atlas.,OWASP LLM AI Security Guide
38,Exploitability of LLM Integrations,Leads to model failure or inaccurate responses.,Access Control,Low,5.6,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"Integrating LLMs with other systems without proper security can lead to vulnerabilities, such as code injection or unauthorized actions.",Secure integration points with authentication and validation; limit the actions that can be performed.,API security tools and middleware for input sanitization.,NIST AI RMF
39,Training Data Replay,Can lead to unauthorized actions or data disclosure.,Configuration Error,High,4.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"An attacker gains access to training data by querying the model and reconstructing the data, potentially exposing sensitive information.",Limit the amount of information the model can output; use differential privacy techniques.,Differential privacy libraries like TensorFlow Privacy.,NIST AI RMF
40,Prompt Injection,Potential disruption of service or data manipulation.,Output Manipulation,High,5.4,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Attackers manipulate AI model prompts to produce unauthorized actions or expose sensitive data.,Implement strict prompt validation and sanitization; restrict the model's output capabilities.,Input validation libraries and AI safety frameworks.,OWASP LLM AI Security Guide
41,External System Integration Risks,Weakens overall security posture of llm deployments.,Model Manipulation,Low,9.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Integrations with third-party systems can introduce vulnerabilities if those systems are compromised, as seen in supply chain attacks.",Vet third-party integrations carefully; monitor and audit external interactions.,Security Information and Event Management (SIEM) systems like Splunk.,NIST AI RMF
42,Oversharing of Data Insights,Exposure of sensitive information or personal data.,API Security,Critical,8.2,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Providing too much detail in analytics can expose sensitive information, similar to how some companies accidentally revealed user data in publicly shared analytics.",Limit the granularity of shared data; use data aggregation and anonymization.,Data masking tools and reporting tools with access controls.,OWASP LLM AI Security Guide
43,Predictive Input Manipulation,Results in data tampering or model corruption.,API Security,Low,4,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Attackers provide inputs designed to manipulate the model's predictions for malicious purposes, like spam emails designed to bypass filters.",Continuously update models with new data; implement adversarial training techniques.,"Machine learning frameworks that support adversarial training, like the CleverHans library.",MITRE ATT&CK for AI
44,Predictable Inference Time,Exposure of sensitive information or personal data.,Model Manipulation,High,7.5,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Timing attacks exploit the time taken to process inputs to infer sensitive information, similar to certain cryptographic attacks.",Implement constant-time processing where feasible; add random delays to processing times.,Code analysis tools to detect timing vulnerabilities.,OWASP LLM AI Security Guide
45,Context Injection Attacks,Leads to model failure or inaccurate responses.,Adversarial Attack,Medium,7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Injecting malicious context into AI models to alter their behavior, similar to prompt injection but at a contextual level.",Validate and sanitize contextual data; restrict model responses to predefined formats.,Context validation libraries and AI safety guidelines.,NIST AI RMF
46,Improper Access Control,Leads to model failure or inaccurate responses.,Adversarial Attack,High,4.1,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Failure to enforce access controls allows unauthorized users to access sensitive functions or data, as seen in some API endpoint vulnerabilities.",Implement role-based access control (RBAC); regularly audit permissions.,Access control systems and IAM tools like AWS IAM or Azure AD.,MITRE ATT&CK for AI
47,Manipulative Model Incentivization,Exposure of sensitive information or personal data.,Adversarial Attack,Low,4.1,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Users manipulate reward signals in reinforcement learning models to achieve unintended outcomes, similar to gaming a system for rewards.",Design robust reward functions; monitor for unexpected agent behaviors.,Reinforcement learning frameworks with monitoring capabilities like OpenAI Gym.,ISO/IEC 27001
48,Input Inference Attacks,Results in data tampering or model corruption.,API Security,Medium,5.8,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Attackers infer sensitive information from model inputs or outputs, such as determining if a person was part of the training data.",Use techniques like differential privacy; limit model output detail.,Privacy-preserving machine learning tools like TensorFlow Privacy.,MITRE ATT&CK for AI
49,Logging Failures,Exposure of sensitive information or personal data.,Data Leakage,Medium,5,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"Improper logging can either miss security incidents or log sensitive data—for example, logging full credit card numbers in plaintext.",Implement proper logging practices with sensitive data masking; monitor logs for anomalies.,Logging tools like ELK Stack with security configurations.,OWASP LLM AI Security Guide
50,Model Entanglement Issues,Leads to model failure or inaccurate responses.,Adversarial Attack,High,8.4,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,"Changes in one part of a model adversely affecting another, leading to unpredictable behavior.",Use modular model architectures; test model components independently.,Model testing frameworks and modular design patterns.,OWASP LLM AI Security Guide
51,Unrestricted Function Calls,Exposure of sensitive information or personal data.,Privacy Violation,Medium,8.2,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Allowing an AI model to execute arbitrary functions can lead to security risks, such as executing system commands.",Limit the functions that the model can call; implement strict code execution policies.,Sandboxing environments and function whitelisting.,OWASP LLM AI Security Guide
52,Overfitting Exploitation,Can lead to unauthorized actions or data disclosure.,Input Manipulation,High,6.2,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Models that overfit may reveal training data when queried in specific ways, potentially exposing sensitive information.",Use regularization techniques during training; validate model performance on unseen data.,Machine learning libraries with regularization options like scikit-learn or TensorFlow.,OWASP LLM AI Security Guide
53,Erroneous LLM Responses,Leads to model failure or inaccurate responses.,Access Control,Medium,8.3,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"LLMs generating incorrect or misleading information, leading to poor decisions based on that information.",Implement human-in-the-loop review for critical outputs; use verification mechanisms.,AI output validation tools and feedback systems.,MITRE ATT&CK for AI
54,Session Hijacking,Results in data tampering or model corruption.,Adversarial Attack,Critical,5,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,"Attackers steal session tokens to impersonate users, as seen in various web application attacks.",Implement secure session management; use HTTPS and secure cookies.,Web frameworks that support secure sessions.,OWASP LLM AI Security Guide
55,Leakage of Inferred Attributes,Leads to model failure or inaccurate responses.,Privacy Violation,Critical,6.9,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Models inferring and revealing sensitive attributes about individuals, such as health conditions from seemingly innocuous data.",Limit the scope of model outputs; use ethical AI guidelines.,Fairness and privacy assessment tools like IBM AI Fairness 360.,NIST AI RMF
56,Prompt Injection,Exposure of sensitive information or personal data.,Output Manipulation,Medium,7.5,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,Attackers manipulate AI model prompts to produce unauthorized actions or expose sensitive data.,Implement strict prompt validation and sanitization; restrict the model's output capabilities.,Input validation libraries and AI safety frameworks.,NIST AI RMF
57,Improper Model Scaling,Weakens overall security posture of llm deployments.,Output Manipulation,Low,8.3,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Scaling models without considering resource limits leads to outages, as seen when services become unavailable during high demand.",Plan for scalability with load testing; use auto-scaling features.,Performance testing tools like JMeter and cloud auto-scaling services.,MITRE ATT&CK for AI
58,Unrestricted Function Calls,Can lead to unauthorized actions or data disclosure.,Privacy Violation,High,7.3,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,Allowing an AI model to execute arbitrary functions can lead to security risks.,Limit the functions that the model can call; implement strict code execution policies.,Sandboxing environments and function whitelisting.,OWASP LLM AI Security Guide
59,Hidden Feedback Channels,Potential disruption of service or data manipulation.,Model Exploitation,High,6.9,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,Attackers exploit feedback mechanisms to alter model behavior without authorization.,Monitor and validate feedback inputs; restrict who can provide feedback.,Secure feedback systems and monitoring tools.,NIST AI RMF
60,Weak Protection for Sensitive Outputs,May allow attackers to alter or control model behavior.,Access Control,High,5.5,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Failure to secure sensitive outputs, such as exposing personal data in AI-generated reports.",Implement output filtering and redaction; use access controls on generated content.,Data masking tools and secure reporting systems.,OWASP LLM AI Security Guide
61,Weak Session Management,Can lead to unauthorized actions or data disclosure.,API Security,Critical,7.6,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Poor session management can lead to session fixation or hijacking, as seen in various web application vulnerabilities.","Use secure, random session identifiers; invalidate sessions after logout or timeout.",Web frameworks with secure session management features.,MITRE ATT&CK for AI
62,Unauthorized Access,Potential disruption of service or data manipulation.,Data Leakage,Low,4.2,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Repetition of unauthorized access, such as when a system's default credentials are not changed.",Enforce strong password policies; disable or change default accounts.,Password management tools and security policies.,ISO/IEC 27001
63,Response Timing Analysis,Potential disruption of service or data manipulation.,Model Exploitation,Low,5.6,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Analyzing the time it takes for a system to respond to infer information, similar to timing attacks on encryption algorithms.",Normalize response times; implement random delays if necessary.,Application Performance Monitoring (APM) tools to detect timing discrepancies.,MITRE ATT&CK for AI
64,Context Injection Attacks,May allow attackers to alter or control model behavior.,Data Leakage,Critical,4.3,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Injecting malicious context into AI models to alter their behavior.,Validate and sanitize contextual data; restrict model responses to predefined formats.,Context validation libraries and AI safety guidelines.,MITRE ATT&CK for AI
65,Model Drift,Potential disruption of service or data manipulation.,API Security,Medium,6.7,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,"Over time, a model's performance degrades as it becomes less representative of current data, leading to inaccurate predictions.",Continuously monitor model performance; retrain models with new data periodically.,Model monitoring tools like Evidently AI or AWS SageMaker Model Monitor.,NIST AI RMF
66,Response Manipulation,Exposure of sensitive information or personal data.,Configuration Error,High,9.5,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Attackers manipulate the model's outputs for malicious purposes, such as generating misleading information.",Validate outputs before they reach the end-user; implement output sanitization.,Output validation libraries and human review processes.,MITRE ATT&CK for AI
67,Hidden Feedback Channels,Can lead to unauthorized actions or data disclosure.,Adversarial Attack,Medium,6,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,Attackers exploit feedback mechanisms to alter model behavior without authorization.,Monitor and validate feedback inputs; restrict who can provide feedback.,Secure feedback systems and monitoring tools.,NIST AI RMF
68,Input Inference Attacks,May allow attackers to alter or control model behavior.,API Security,Critical,7.8,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,Attackers infer sensitive information from model inputs or outputs.,Use techniques like differential privacy; limit model output detail.,Privacy-preserving machine learning tools like TensorFlow Privacy.,ISO/IEC 27001
69,Hidden Feedback Channels,Potential disruption of service or data manipulation.,Configuration Error,High,7.9,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Repetition of hidden feedback channel exploitation.,Monitor and validate feedback inputs; restrict who can provide feedback.,Secure feedback systems and monitoring tools.,NIST AI RMF
70,Invisible Prompt Attack,May allow attackers to alter or control model behavior.,Access Control,Low,6,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Embedding hidden prompts in user inputs that alter the model's behavior without detection.,Filter and sanitize all inputs; use techniques to detect and remove hidden content.,Input validation libraries and content filtering tools.,MITRE ATT&CK for AI
71,Undetected Model Modification,Exposure of sensitive information or personal data.,Privacy Violation,High,5.6,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,An attacker subtly alters a model to produce incorrect outputs.,Implement integrity checks and version control for models; use cryptographic hashes to verify models.,Version control systems like Git and checksum tools like SHA256 verification.,OWASP LLM AI Security Guide
72,Context Injection Attacks,Potential disruption of service or data manipulation.,Privacy Violation,Medium,6.4,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Injecting malicious context into AI models.,Validate and sanitize contextual data; restrict model responses to predefined formats.,Context validation libraries and AI safety guidelines.,ISO/IEC 27001
73,Overshadowing Vulnerability,Weakens overall security posture of llm deployments.,Data Leakage,Medium,4.7,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Attackers exploit overlapping input sequences to confuse the model, leading to incorrect outputs.",Implement input disambiguation techniques; enforce strict input parsing rules.,Input validation libraries and robust parsing algorithms.,ISO/IEC 27001
74,Privacy Policy Violation,Exposure of sensitive information or personal data.,Model Manipulation,Critical,6.4,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Not adhering to privacy policies, leading to legal actions.",Regularly review and comply with privacy policies; train staff on privacy practices.,Privacy compliance tools and legal consultation services.,NIST AI RMF
75,Exposed Data During Debugging,Leads to model failure or inaccurate responses.,Model Manipulation,Medium,4.7,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,Sensitive data is exposed through debug logs or error messages during development.,Disable debug logs in production; sanitize error messages.,"Logging frameworks with production settings, like Log4j.",MITRE ATT&CK for AI
76,Poisoned Model Updates,Can lead to unauthorized actions or data disclosure.,Output Manipulation,Low,8.6,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,"Attackers introduce malicious updates to the model, affecting its behavior.",Validate and test model updates before deployment; secure the update pipeline.,"CI/CD tools with security checks, like Jenkins with security plugins.",MITRE ATT&CK for AI
77,Unrestricted LLM Functions,Can lead to unauthorized actions or data disclosure.,Input Manipulation,Low,6.9,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"Allowing an LLM to execute arbitrary code, leading to potential exploits.",Limit the functionalities of the LLM; implement execution sandboxing.,Secure sandbox environments like Docker containers.,ISO/IEC 27001
78,Backdoor Attacks,Exposure of sensitive information or personal data.,Model Manipulation,High,8,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,Attackers insert backdoors into models to allow future unauthorized access.,Validate and test models for unexpected behavior; use secure development practices.,Code analysis tools and security audits.,MITRE ATT&CK for AI
79,Response Timing Analysis,Results in data tampering or model corruption.,Data Leakage,Critical,9.7,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Timing attacks to infer sensitive information.,Normalize response times; implement random delays if necessary.,Application Performance Monitoring (APM) tools to detect timing discrepancies.,ISO/IEC 27001
80,Predictable Inference Time,Potential disruption of service or data manipulation.,Configuration Error,Medium,9,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,Similar to response timing analysis; attackers use inference time to deduce information.,Implement constant-time processing; add random delays.,Code analysis tools.,MITRE ATT&CK for AI
81,Oversharing of Data Insights,Can lead to unauthorized actions or data disclosure.,Model Manipulation,Medium,9.5,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,Providing too much detail in analytics can expose sensitive information.,Limit data granularity; use data aggregation and anonymization.,Data masking tools and reporting tools with access controls.,NIST AI RMF
82,Cache Poisoning,Weakens overall security posture of llm deployments.,Data Leakage,Low,5.8,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,An attacker poisons the cache with malicious data.,Validate cached data integrity; implement cache eviction policies.,Secure caching solutions like Redis with security configurations.,MITRE ATT&CK for AI
83,Data Retention Issues,Can lead to unauthorized actions or data disclosure.,Adversarial Attack,Medium,4.2,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Storing data longer than necessary increases risk.,Implement data retention and deletion policies; automate data lifecycle management.,Data management tools like Apache Atlas.,ISO/IEC 27001
84,Model Drift,Results in data tampering or model corruption.,Configuration Error,Critical,5.8,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Model performance degrades over time due to changing data patterns.,Continuously monitor model performance; retrain models periodically.,Model monitoring tools like Evidently AI.,OWASP LLM AI Security Guide
85,Lack of Governance,May allow attackers to alter or control model behavior.,Configuration Error,Medium,7,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,"Without proper AI governance, models may be deployed with biases or legal issues.",Establish AI governance frameworks; conduct regular audits.,AI governance platforms like IBM OpenPages.,ISO/IEC 27001
86,Excessive Data Collection,Exposure of sensitive information or personal data.,Access Control,Low,6.6,"GPT-3, GPT-4, PaLM, LLaMA",Leads to model failure or inaccurate responses.,Collecting more user data than necessary leads to privacy concerns.,Collect only necessary data; be transparent with users.,Compliance management tools like TrustArc.,MITRE ATT&CK for AI
87,Backend Server Exploitation,May allow attackers to alter or control model behavior.,Input Manipulation,Low,9.2,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Exploiting vulnerabilities in backend servers supporting AI models.,Keep servers updated; use firewalls and intrusion detection systems.,Security tools like Snort IDS or OSSEC.,NIST AI RMF
88,Overfitting Exploitation,Potential disruption of service or data manipulation.,API Security,Low,6.3,"GPT-3, GPT-4, PaLM, LLaMA",Results in data tampering or model corruption.,Models that overfit may reveal training data when queried.,Use regularization; validate on unseen data.,Machine learning libraries like scikit-learn.,ISO/IEC 27001
89,Context Injection Attacks,Can lead to unauthorized actions or data disclosure.,Input Manipulation,High,7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Injecting malicious context into AI models.,Validate and sanitize contextual data; restrict responses.,Context validation libraries.,ISO/IEC 27001
90,Weak Protection for Sensitive Outputs,Exposure of sensitive information or personal data.,Access Control,Critical,9.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"Failure to secure sensitive outputs, such as exposing personal data.",Implement output filtering; use access controls.,Data masking tools and secure reporting systems.,MITRE ATT&CK for AI
91,Denial of Service,Weakens overall security posture of llm deployments.,Model Exploitation,Medium,8.6,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,Overloading a service to make it unavailable to legitimate users.,Implement network traffic monitoring; use DDoS protection services.,Services like Cloudflare DDoS Protection or AWS Shield.,MITRE ATT&CK for AI
92,Sensitive Data Exfiltration,Leads to model failure or inaccurate responses.,Configuration Error,Critical,5.7,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Unauthorized extraction of sensitive data from a system.,Monitor for unusual data access; implement DLP measures.,DLP solutions like McAfee DLP.,OWASP LLM AI Security Guide
93,Response Manipulation,Weakens overall security posture of llm deployments.,Adversarial Attack,Low,8.9,"GPT-3, GPT-4, PaLM, LLaMA",Potential disruption of service or data manipulation.,Attackers manipulate the model's outputs for malicious purposes.,Validate outputs; implement output sanitization.,Output validation libraries.,MITRE ATT&CK for AI
94,Invisible Prompt Attack,Results in data tampering or model corruption.,Model Manipulation,Medium,8.6,"GPT-3, GPT-4, PaLM, LLaMA",Can lead to unauthorized actions or data disclosure.,Embedding hidden prompts in user inputs.,Filter and sanitize inputs; detect hidden content.,Input validation libraries and content filtering tools.,NIST AI RMF
95,Cross-Tenant Attacks,Weakens overall security posture of llm deployments.,Data Leakage,Low,6.3,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,"In multi-tenant systems, one tenant accesses another's data.",Enforce data isolation policies; use tenant-aware access controls.,Multi-tenancy frameworks and IAM solutions.,MITRE ATT&CK for AI
96,Weak Model Parameter Encryption,Potential disruption of service or data manipulation.,Data Leakage,Critical,6.5,"GPT-3, GPT-4, PaLM, LLaMA",Exposure of sensitive information or personal data.,"If model parameters are stored without encryption, attackers can steal the model.",Encrypt model parameters; use secure key management.,Encryption libraries like PyCrypto or AWS KMS.,MITRE ATT&CK for AI
97,External System Integration Risks,Weakens overall security posture of llm deployments.,Data Leakage,Critical,5,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Integrations with compromised third-party systems introduce vulnerabilities.,Vet third-party integrations; monitor external interactions.,Security Information and Event Management (SIEM) systems like Splunk.,MITRE ATT&CK for AI
98,Incorrect Model Configuration,Results in data tampering or model corruption.,Privacy Violation,Medium,4.6,"GPT-3, GPT-4, PaLM, LLaMA",May allow attackers to alter or control model behavior.,Misconfigured models can expose debug information.,Follow configuration management best practices; secure environment variables.,Configuration management tools like Ansible.,NIST AI RMF
99,Improper Input Filtering,Potential disruption of service or data manipulation.,Model Exploitation,Critical,9.4,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Failure to properly filter inputs can lead to injection attacks.,Implement comprehensive input validation; use allowlists.,Security libraries like OWASP ESAPI.,NIST AI RMF
100,Insufficient Encryption,Potential disruption of service or data manipulation.,Configuration Error,High,9.3,"GPT-3, GPT-4, PaLM, LLaMA",Weakens overall security posture of LLM deployments.,Using weak encryption algorithms makes data susceptible to interception.,Use strong encryption algorithms; regularly update protocols.,Encryption libraries like OpenSSL with strong cipher suites.,NIST AI RMF
